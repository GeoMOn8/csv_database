{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9749c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd part of the project, write a bunch of functions to completely script the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f1f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# conda install psycopg2\n",
    "# pip install psycopg2\n",
    "# pip install mysql-connector-python\n",
    "\n",
    "import os # interaction with operating system\n",
    "import numpy as np # to work with arrays\n",
    "import pandas as pd # multi-dimentional arrays\n",
    "import psycopg2 # postgres wrapper, make a connection to database\n",
    "#import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946f268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate only the CSV files\n",
    "# write a for loop to check each item and only retrive those that is a csv\n",
    "\n",
    "def csv_files():\n",
    "    \n",
    "    csv_files = [] # initialize empty file list\n",
    "    for file in os.listdir(os.getcwd()):# kinda the same as ls but now its saved in a list\n",
    "    \n",
    "        if file.endswith('.csv'):\n",
    "        \n",
    "            csv_files.append(file)\n",
    "            \n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89ea3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['us_cities.csv',\n",
       " 'average-latitude-longitude-countries.csv',\n",
       " 'world_cities.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7e005c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new directory\n",
    "\n",
    "def configure_dataset_directory(csv_files, dataset_dir):\n",
    "\n",
    "    # create the bash command to make a new directory\n",
    "    # mkdir dataset_dir\n",
    "\n",
    "    # if the folder already exist, ignore\n",
    "    # if you don't have sudo permission to make a directory, it won't work\n",
    "    try:\n",
    "\n",
    "        mkdir = 'mkdir {0}'.format(dataset_dir) #\n",
    "        os.system(mkdir)\n",
    "    except:\n",
    "        print('File aleady created')\n",
    "        \n",
    "    # move the CSV files in the new directory\n",
    "\n",
    "    # mv filename directory\n",
    "    for csv in csv_files:\n",
    "\n",
    "        mv_file = \"mv '{0}' {1}\".format(csv, dataset_dir) # the qoutes here help python to read the file name as a single file incase there's afile with a space\n",
    "        os.system(mv_file)\n",
    "        print(mv_file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5beb771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autmate the file reading process and also be able to handle multiple csv's\n",
    "# replace manual typing of csv's names with the path and file name\n",
    "# create a dictionary to hold all the dataframes\n",
    "# define the path to the csv file\n",
    "# for loop to get the file name, read each as a panda dataframe\n",
    "def create_df(dataset_dir, csv_files):\n",
    "    \n",
    "    # path to the csv files\n",
    "    data_path = os.getcwd() + '/' + dataset_dir + '/'\n",
    "    \n",
    "    # loop through the files and create the dataframe\n",
    "    data = {}\n",
    "    for file in csv_files:\n",
    "\n",
    "        try:\n",
    "            data[file] = pd.read_csv(data_path+file)\n",
    "        except UnicodeDecodeError:\n",
    "            data[file] = pd.read_csv(data_path+file, encoding=\"ISO-8859-1\")\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d3ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data name if need be\n",
    "# lower case letter\n",
    "# remove all white spaces\n",
    "# replace -, /, \\\\, $ with _\n",
    "\n",
    "# write script that will enforce the rules above\n",
    "# using escape \"/\" at the end og my code line to go to the next line instead of havinh one very long line of code (clean code)\n",
    "# using \"r\" raw string, use the symbol in it's raw\n",
    "\n",
    "def clean_tbl_name(filename):\n",
    "    \n",
    "    clean_tbl_name = filename.lower().replace(\" \",\"_\").replace(\"?\",\"\") \\\n",
    "                    .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\") \\\n",
    "                    .replace(r\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\")\n",
    "    \n",
    "    # remove .csv extention from clean_tbl_name\n",
    "    \n",
    "    tbl_name = '{0}'.format(clean_tbl_name.split('.')[0])\n",
    "    \n",
    "    return tbl_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d24783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_clm_name(dataframe):\n",
    "    \n",
    "    # write list comprehension to clean up names\n",
    "\n",
    "    dataframe.columns = [x.lower().replace(\" \",\"_\").replace(\"?\",\"\") \\\n",
    "                    .replace(\"-\",\"_\").replace(r\"/\",\"_\").replace(\"\\\\\",\"_\").replace(\"%\",\"\") \\\n",
    "                    .replace(r\")\",\"\").replace(r\"(\",\"\").replace(\"$\",\"\") for x in dataframe.columns]\n",
    "    \n",
    "    # SQL data types don't match the pandas data types\n",
    "    # replace the pandas data types with SQL data types, using a dictionary\n",
    "    \n",
    "    # because this script can be run with 'any' cvs file - I am going genaral with the replacements - feel free to add\n",
    "    \n",
    "    replacements = {\n",
    "    'object': 'varchar',\n",
    "    'float64': 'double precision',\n",
    "    'int64': 'int',\n",
    "    'datetime64': 'timestamp',\n",
    "    'timedelta64[ns]': 'varchar'\n",
    "    }\n",
    "    \n",
    "    # comma will seperate the lines(in the sql statement, join the comma wit the column name and data type (thats the curly brackets\n",
    "    # format i to place the column names and data types respectively, for loop to go through each element in the list(data.lolumns and data.dtypes\n",
    "    # dtypes = panda datatypes, replace those with sql data types by using the replace function using the replacements dictionary \n",
    "    # zip everything up, want the column names next to the dtypes names\n",
    "    \n",
    "    # table schema\n",
    "\n",
    "    clm_str = \", \".join(\"{} {}\".format(n, d) for (n, d) in zip(dataframe.columns, dataframe.dtypes.replace(replacements)))\n",
    "    \n",
    "    return clm_str, dataframe.columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e239d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create database (dbBeaver)\n",
    "# Connect to database\n",
    "# connect_str = \"host=jdbc:postgresql://localhost:5432/project_data \\\n",
    "#            dbname='project_data' \\\n",
    "#            user='postgres' pasword='post88'\n",
    "\n",
    "# establish connection\n",
    "    \n",
    "def upload_to_db(host, database, user, password, tbl_name, clm_str, file, dataframe, dataframe_columns):\n",
    "    \n",
    "    conn_string = \"host=%s database=%s user=%s password=%s\" % (host, dbname, user, password)\n",
    "    connect = psycopg2.connect(conn_string)\n",
    "\n",
    "    cursor = connect.cursor()\n",
    "\n",
    "    print(\"opened DB success\")\n",
    "    # drop tables with same name\n",
    "    cursor.execute(\"drop table if exists %s;\" % (tbl_name))\n",
    "    \n",
    "    # create table\n",
    "    cursor.execute(\"create table %s (%s);\" % (tbl_name, clm_str))\n",
    "    print('{0} was created successfully'.format(tbl_name))\n",
    "    \n",
    "    #connect.close()\n",
    "    \n",
    "    # insert values to table\n",
    "\n",
    "    #save pandas data-frame to csv file\n",
    "    dataframe.to_csv(file , header=dataframe_columns, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "    # open csv file, save it as an object, and upload to db\n",
    "    # taking the csv file, opoen it up in the ram, then preparing it to insert into database table\n",
    "    mon_file = open(file)\n",
    "    print(\"filed opened in memory\")\n",
    "    \n",
    "    # upload to db\n",
    "\n",
    "    SQL_STATEMENT = \"\"\"\n",
    "    COPY %s from STDIN WITH\n",
    "        CSV\n",
    "        HEADER\n",
    "        DELIMITER AS ','\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.copy_expert(sql=SQL_STATEMENT % tbl_name, file=mon_file)\n",
    "    print(\"file copied to db\")\n",
    "    \n",
    "    #grant multiple user access to table to anyone who has access to my database\n",
    "\n",
    "    cursor.execute(\"grant select on %s to public\" % tbl_name)\n",
    "    connect.commit()\n",
    "\n",
    "    cursor.close()\n",
    "    print(\"table {0} to db completed\".format(tbl_name))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe7708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
